# Response Mask Guide - The Most Critical Part of Remote Rollout

**Status**: CRITICAL READING

## Table of Contents

1. [Why Response Mask Matters](#why-response-mask-matters)
2. [What is response_mask](#what-is-response_mask)
3. [Training Data Corruption Examples](#training-data-corruption-examples)
4. [Correct Implementation Pattern](#correct-implementation-pattern)
5. [Common Pitfalls](#common-pitfalls)
6. [Debugging Incorrect Masks](#debugging-incorrect-masks)
7. [Testing Mask Correctness](#testing-mask-correctness)

---

üëâ Looking for a concrete payload walkthrough? See `docs/REQUEST_FLOW_EXAMPLE.md` for a turn-by-turn `/rollout` ‚Üî `/v1/chat/completions` example using the mock trainer.

## Why Response Mask Matters

**The `response_mask` is THE most critical part of the Remote Rollout Protocol.**

During PPO (Proximal Policy Optimization) training, the training cluster needs to know which tokens were generated by the LLM (and should be used to compute the policy gradient) versus which tokens are tool outputs or system messages (and should be excluded from training).

**Without correct masks**:
- ‚ùå Training data gets corrupted
- ‚ùå PPO loss is calculated on wrong tokens
- ‚ùå Model learns incorrect behaviors
- ‚ùå Training diverges or stagnates

**This is not a performance issue - it's a correctness issue.**

---

## What is response_mask

`response_mask` is a list of integers (0 or 1) that indicates the "source" of each token in the conversation:

- **`mask = 1`**: Token was **generated by the LLM** (participates in PPO training loss)
- **`mask = 0`**: Token is **tool/system output** (excluded from training loss)

### Example: Multi-Turn Conversation

```
User: "Calculate 15 * 23"

Turn 1:
  LLM generates: "I'll use the calculator tool"
  Tokens: [4, 5, 6, 7, 8]
  Mask:   [1, 1, 1, 1, 1]  ‚Üê All LLM-generated

  LLM calls tool: calculator.multiply(15, 23)

Turn 2:
  Tool returns: "345"
  Tokens: [9, 10]
  Mask:   [0, 0]  ‚Üê Tool output, NOT LLM-generated

  LLM generates: "The result is 345"
  Tokens: [11, 12, 13, 14, 15]
  Mask:   [1, 1, 1, 1, 1]  ‚Üê All LLM-generated

Final response_ids:  [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
Final response_mask: [1, 1, 1, 1, 1, 0, 0,  1,  1,  1,  1,  1]
                      ‚îî LLM tokens‚îò ‚îîTool‚îò  ‚îî‚îÄ‚îÄ LLM tokens ‚îÄ‚îò
```

**Key Insight**: The mask distinguishes between what the model said (`1`) and what external systems said (`0`).

---

## Training Data Corruption Examples

### Example 1: False Positives (LLM tokens marked as 0)

**Incorrect Mask**:
```
response_ids:  [4, 5, 6, 7, 8, 9, 10, 11, 12]
response_mask: [1, 1, 1, 0, 0, 0,  0,  0,  0]  ‚Üê Wrong! Marked LLM tokens as 0
               ‚îî‚îÄ LLM ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ LLM tokens ‚îÄ‚îÄ‚îò
                         ‚ö†Ô∏è  Should be 1s, not 0s!
```

**Consequence**:
- Training loss excludes tokens 7-12
- Model can't learn from its decisions in that part of the conversation
- **Training stagnates** - model stops improving

### Example 2: False Negatives (Tool tokens marked as 1)

**Incorrect Mask**:
```
response_ids:  [4, 5, 6, 7, 8, 9, 10, 11, 12]
response_mask: [1, 1, 1, 1, 1, 1,  1,  1,  1]  ‚Üê Wrong! Marked tool tokens as 1
               ‚îî‚îÄ LLM ‚îÄ‚îò ‚îîTool‚îò ‚îî‚îÄ‚îÄ LLM ‚îÄ‚îÄ‚îò
                         ‚ö†Ô∏è  Should be 0s!
```

**Consequence**:
- Training loss includes tool output tokens 9-10
- Model tries to learn to "predict" tool outputs
- **Training diverges** - model learns wrong objective

### Example 3: Mask Length Mismatch

**Incorrect Mask**:
```
response_ids:  [4, 5, 6, 7, 8, 9, 10, 11, 12]  (9 tokens)
response_mask: [1, 1, 1, 0, 0]                   (5 values) ‚Üê Wrong length!
```

**Consequence**:
- Trainer can't align mask with tokens
- Falls back to default mask (all 1s) or crashes
- **Data corruption or training failure**

---

## Correct Implementation Pattern

### The Reference Pattern

```python
class RolloutSession:
    """Manages rollout session with CORRECT response_mask tracking."""

    def __init__(self, rollout_id: str, tokenizer, server_url: str):
        self.rollout_id = rollout_id
        self.tokenizer = tokenizer
        self.server_url = server_url
        self.messages = []
        self.last_prompt_length = 0  # CRITICAL for mask calculation

    async def call_llm(self, sampling_params: dict):
        """Call LLM with CORRECT response_mask calculation."""

        # 1. Tokenize current messages to get prompt length
        current_prompt = self.tokenizer.apply_chat_template(
            self.messages, add_generation_prompt=True, tokenize=True
        )
        current_prompt_length = len(current_prompt)

        # 2. Calculate mask for tokens added since last call (CRITICAL!)
        if self.last_prompt_length > 0:
            # Tokens added between calls = tool outputs from previous turn
            num_new_tokens = current_prompt_length - self.last_prompt_length
            response_mask = [0] * num_new_tokens if num_new_tokens > 0 else None
        else:
            # First turn - no tool outputs yet
            response_mask = None

        # 3. Call trainer with EXPLICIT mask
        response = await httpx.post(
            f"{self.server_url}/v1/chat/completions",
            json={
                "rollout_id": self.rollout_id,
                "messages": self.messages,
                "response_mask": response_mask,  # CRITICAL!
                **sampling_params
            }
        )

        # 4. Update tracking with LLM response length
        llm_token_count = len(response["token_ids"])
        self.last_prompt_length = current_prompt_length + llm_token_count

        return response

    def append_tool_outputs(self, tool_results: list):
        """Append tool results. Don't update last_prompt_length here!"""
        self.messages.extend(tool_results)
        # ‚ö†Ô∏è  IMPORTANT: Don't update last_prompt_length here!
        #    It will be calculated in next call_llm()
```

### Why This Works

**Key Mechanism**: Track `last_prompt_length` between LLM calls.

1. **Before each LLM call**: Tokenize current messages ‚Üí get `current_prompt_length`
2. **Calculate diff**: `num_new_tokens = current_prompt_length - last_prompt_length`
3. **These new tokens are tool outputs**: Mark them with `mask = 0`
4. **After LLM call**: Update `last_prompt_length = current_prompt_length + llm_token_count`

**Visualization**:

```
Turn 1:
  last_prompt_length = 0 (initial)
  Tokenize messages ‚Üí current_prompt = [1, 2, 3] (user message)
  num_new_tokens = 3 - 0 = 3 (but first turn, so response_mask = None)
  Call LLM ‚Üí response = [4, 5, 6]
  Update: last_prompt_length = 3 + 3 = 6

Turn 2:
  Append tool output: "345"
  Tokenize messages ‚Üí current_prompt = [1, 2, 3, 4, 5, 6, 7, 8]
  num_new_tokens = 8 - 6 = 2 (tool output tokens [7, 8])
  response_mask = [0, 0]  ‚Üê Mark tool tokens!
  Call LLM ‚Üí response = [9, 10, 11]
  Update: last_prompt_length = 8 + 3 = 11
```

---

## Common Pitfalls

### Pitfall 1: Forgetting response_mask After Tool Execution

‚ùå **Wrong**:
```python
# Execute tools
messages.extend(tool_results)

# Call LLM again - missing response_mask!
response = await post(f"{server_url}/v1/chat/completions", json={
    "rollout_id": rollout_id,
    "messages": messages,
    # response_mask is missing!
})
```

**Consequence**: Trainer falls back to diff-based inference, which breaks if you ever modify context.

‚úÖ **Correct**:
```python
# Calculate mask for tool outputs
num_tool_tokens = len(tokenize_tool_results(tool_results))
response_mask = [0] * num_tool_tokens

# Call LLM with explicit mask
response = await post(f"{server_url}/v1/chat/completions", json={
    "rollout_id": rollout_id,
    "messages": messages,
    "response_mask": response_mask,  # ‚úì
})
```

### Pitfall 2: Wrong Mask Timing

‚ùå **Wrong**:
```python
# Turn 1: Call LLM
response1 = await call_llm(messages, response_mask=[0,0,0])  # ‚ùå No tool outputs yet!

# Execute tools
messages.extend(tool_results)

# Turn 2: Call LLM
response2 = await call_llm(messages, response_mask=None)  # ‚ùå Should provide mask here!
```

‚úÖ **Correct**:
```python
# Turn 1: No previous tool outputs
response1 = await call_llm(messages, response_mask=None)  # ‚úì

# Execute tools
messages.extend(tool_results)

# Turn 2: Provide mask for tool outputs from Turn 1
response2 = await call_llm(messages, response_mask=[0,0,0])  # ‚úì
```

**Key Insight**: `response_mask` describes tokens added **before** the current call, not **after**.

### Pitfall 3: Mask Length Mismatch

‚ùå **Wrong**:
```python
# Tool outputs tokenize to 150 tokens
tool_tokens = tokenizer.encode(tool_output_text)  # len=150

# But you provide wrong count
response_mask = [0] * 100  # ‚ùå Wrong length!
```

**Consequence**: Trainer logs warning and falls back to default mask (all 1s), breaking training.

‚úÖ **Correct**:
```python
# Calculate exact token count
tool_tokens = tokenizer.encode(tool_output_text)
response_mask = [0] * len(tool_tokens)  # ‚úì Exact match
```

### Pitfall 4: Tokenizer Misalignment

‚ùå **Wrong**:
```python
# RolloutServer uses different tokenizer
rollout_tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

# Trainer uses Qwen tokenizer
trainer_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

# Token IDs won't match! Training will crash or diverge.
```

‚úÖ **Correct**:
```python
# Use EXACT same tokenizer
# Note: Qwen3 requires trust_remote_code=True for custom tokenizer code
tokenizer_name = request["tokenizer_name"]  # From rollout request
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
```

### Pitfall 5: Context Truncation Without Explicit Mask

‚ùå **Wrong**:
```python
# Turn 1: messages = [msg1, msg2, msg3]
response1 = await call_llm(messages)

# Turn 2: Truncate old messages
messages = messages[-5:]  # ‚ùå Breaks token tracking!

# Call LLM without explicit mask
response2 = await call_llm(messages)  # ‚ùå Diff logic will fail!
```

‚úÖ **Correct**:
```python
# If you must truncate, provide explicit mask
messages = messages[-5:]

# Calculate which tokens are tool outputs (you need to track this!)
tool_token_count = calculate_tool_tokens_in_truncated_context(messages)
response_mask = [0] * tool_token_count

response2 = await call_llm(messages, response_mask=response_mask)  # ‚úì
```

**Better**: Avoid truncation if possible, or use summarization with explicit token tracking.

---

## Debugging Incorrect Masks

### Enable Detailed Logging

```python
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("rollout_server")

# Log mask calculations
logger.debug(f"Turn {turn}: response_mask={response_mask}, length={len(response_mask)}")
logger.debug(f"Tool output tokens: {tool_token_ids}")
logger.debug(f"Prompt length before: {last_prompt_length}, after: {current_prompt_length}")
```

### Verify Token Alignment

```python
# After each LLM call, verify token counts match
response = await call_llm(...)

# Log for manual inspection
logger.info(f"Prompt token count: {len(response['prompt_token_ids'])}")
logger.info(f"Response token count: {len(response['token_ids'])}")
logger.info(f"Expected prompt length: {current_prompt_length}")

assert len(response['prompt_token_ids']) == current_prompt_length, \
    "Token count mismatch - tokenizer alignment issue!"
```

### Inspect Session State

```python
# After rollout completes, verify mask structure
session = trainer.get_session(rollout_id)

print(f"Total response tokens: {len(session.response_ids)}")
print(f"Mask length: {len(session.response_mask)}")
print(f"LLM tokens (mask=1): {sum(1 for m in session.response_mask if m == 1)}")
print(f"Tool tokens (mask=0): {sum(1 for m in session.response_mask if m == 0)}")

# Verify mask pattern
assert len(session.response_ids) == len(session.response_mask), "Length mismatch!"
assert all(m in [0, 1] for m in session.response_mask), "Invalid mask values!"
```

---

## Testing Mask Correctness

### Test Pattern: Single Turn (No Tools)

```python
async def test_single_turn_no_tools():
    """Verify single-turn conversation has correct masks."""
    response = await rollout_server.rollout({
        "rollout_id": "test-1",
        "messages": [{"role": "user", "content": "Hello"}],
        ...
    })

    session = trainer.get_session("test-1")

    # All tokens should be LLM-generated (mask=1)
    assert all(m == 1 for m in session.response_mask)
```

### Test Pattern: Multi-Turn with Tools

```python
async def test_multi_turn_with_tools():
    """CRITICAL: Verify multi-turn masks are CORRECT."""
    response = await rollout_server.rollout({
        "rollout_id": "test-2",
        "messages": [{"role": "user", "content": "Calculate 15*23"}],
        ...
    })

    session = trainer.get_session("test-2")

    # Expected pattern: [LLM tokens, tool output tokens, LLM tokens]
    # Mask should be: [1,1,1,...,0,0,0,...,1,1,1]

    assert 0 in session.response_mask, "Should have tool tokens (mask=0)"
    assert 1 in session.response_mask, "Should have LLM tokens (mask=1)"

    # Count tool output tokens
    tool_token_count = count_tool_output_tokens(response.final_messages)
    mask_zero_count = session.response_mask.count(0)

    assert mask_zero_count == tool_token_count, \
        f"Tool token mismatch: expected {tool_token_count}, got {mask_zero_count}"
```

### Test Pattern: Mask Length Verification

```python
async def test_mask_length_matches_tokens():
    """Verify mask length always matches token count."""
    response = await rollout_server.rollout({...})

    session = trainer.get_session(rollout_id)

    # CRITICAL: Lengths must match
    assert len(session.response_ids) == len(session.response_mask), \
        f"Length mismatch: {len(session.response_ids)} tokens, {len(session.response_mask)} mask values"
```

---

## Summary: Critical Rules

1. **Use the exact same tokenizer as the trainer** (model ID + revision)
2. **Provide explicit `response_mask` for every call after Turn 1**
3. **Calculate mask by tokenizing tool outputs** (don't guess!)
4. **Mask semantics**: `0` = tool/system tokens, `1` = LLM tokens
5. **Always include `rollout_id`** in `/v1/chat/completions` requests
6. **Track `last_prompt_length`** between LLM calls (key to correct masks)
7. **Test mask correctness** with integration tests

## What Happens If You Don't Follow This?

- Training data corruption (wrong masks)
- PPO training divergence or stagnation
- Silent failures (model learns wrong behaviors)
- Non-reproducible results
- Hours/days of wasted GPU time

**Follow this guide to ensure correct, robust training.**

---

## References

- **Reference Implementation**: `src/rollout_server/session.py`
